= CRUD Document Operations Using the Java SDK with Couchbase Server
:navtitle: Document Operations
:page-topic-type: concept
:page-aliases: documents-creating,documents-updating,documents-retrieving,documents-deleting,howtos:kv-operations.adoc

[abstract]
You can access documents in Couchbase using methods of the [.api]`couchbase.couchbase.client.java.Bucket` object.

The methods for retrieving documents are [.api]`get()` and [.api]`lookupIn()` and the methods for mutating documents are [.api]`upsert()`, [.api]`insert()`, [.api]`replace()` and [.api]`mutateIn()`.

Examples are shown using the synchronous API.
See the section on xref:async-programming.adoc[Async Programming] for other APIs.

[#java-additional-options]
== Additional Options

Update operations also accept a xref:core-operations.adoc#expiry[TTL (expiry)] value ([.param]`expiry`) on the passed document which will instruct the server to delete the document after a given amount of time.
This option is useful for transient data (such as sessions).
By default documents do not expire.
See xref:core-operations.adoc#expiry[Expiration Overview] for more information on expiration.

Update operations can also accept a xref:concurrent-mutations-cluster.adoc[CAS] ([.param]`cas`) value on the passed document to protect against concurrent updates to the same document.
See xref:concurrent-mutations-cluster.adoc[CAS] for a description on how to use CAS values in your application.
Since CAS values are opaque, they are normally retreived when a Document is loaded from Couchbase and then used subsequently (without modification) on the mutation operations.
If a mutation did succeed, the returned Document will contain the new CAS value.

[#java-mutation-input]
== Document Input and Output Types

Couchbase stores documents.
From an SDK point of view, those documents contain the actual value (like a JSON object) and associated metadata.
Every document in the Java SDK contains the following properties, some of them optional depending on the context:

|===
| Name | Description

| `id`
| The (per bucket) unique identifier of the document.

| `content`
| The actual content of the document.

| `cas`
| The CAS (Compare And Swap) value of the document.

| `expiry`
| The expiration time of the document.

| `mutationToken`
| The optional MutationToken after a mutation.
|===

There are a few different implementations of a `Document`.
Here are a few noteworthy document types:

* [.api]`JsonDocument`: The default one in most methods, contains a JSON object (as a `JsonObject`).
* [.api]`RawJsonDocument`: Represents any JSON value, stored as a `String` (useful for when you have your own JSON serializer/deserializer).
* [.api]`BinaryDocument`: Used to store pure raw binary data (as a `ByteBuf` from Netty).
+
IMPORTANT: The `ByteBuf` comes from Netty, and when reading one from the SDK, you need to manage its memory by hand by calling `release()`.
See <<java-binary-document,the section about binary documents>>.

Because Couchbase Server can store anything and not just JSON files, many document types exist to satisfy the general needs of an application.
You can also write your own `Document` implementations, which is not covered in this introduction.

[#java-creating-updating-full-docs]
== Creating and Updating Full Documents

Documents may be created and updated using the [.api]`Bucket#upsert()`, [.api]`Bucket#insert()`, and [.api]`Bucket#replace()` family of methods.
Read more about the difference between these methods at xref:core-operations.adoc#crud-overview[Primitive Key-Value Operations] in the Couchbase developer guide.

These methods accept a Document instance where the following values are considered if set:

* [.param]`id` (mandatory): The ID of the document to modify (String).
* [.param]`content` (mandatory): The desired new content of the document, this varies per document type used.
If the `JsonDocument` is used, the document type is a `JsonObject`.
* [.param]`expiry` (optional): Specify the expiry time for the document.
If specified, the document will expire and no longer exist after the given number of seconds.
See xref:core-operations.adoc#expiry[Expiration Overview] for more information.
* [.param]`cas` (optional): The CAS value for the document.
If the CAS on the server does not match the CAS supplied to the method, the operation will fail with a [.api]`CASMismatchException`.
See xref:concurrent-mutations-cluster.adoc[Concurrent Document Mutations] for more information on the usage of CAS values.

Other optional arguments are also available for more advanced usage:

* [.param]`persistTo`, [.param]`replicateTo`: Specify xref:durability.adoc[durability requirements] for the operations.
* [.param]`timeout`, [.param]`timeUnit`: Specify a custom timeout which overrides the default timeout setting.

Upon success, the returned [.api]`Document` instance will contain the new xref:concurrent-mutations-cluster.adoc[CAS] value of the document.
If the document is not mutated successfully, an exception is raised depending on the type of error.

Inserting a document works like this:

[source,java]
----
JsonDocument doc = JsonDocument.create("document_id", JsonObject.create().put("some", "value"));
System.out.println(bucket.insert(doc));
----

....
Output: JsonDocument{id='document_id', cas=216109389250560, expiry=0, content={"some":"value"}, mutationToken=null}
....

If the same code is called again, a `DocumentAlreadyExistsException` will be thrown.
If you don't care that the document is overridden, you can use [.api]`upsert` instead:

[source,java]
----
JsonDocument doc = JsonDocument.create("document_id", JsonObject.empty().put("some", "other value"));
System.out.println(bucket.upsert(doc));
----

....
Output: JsonDocument{id='document_id', cas=216109392920576, expiry=0, content={"some":"other value"}, mutationToken=null}
....

Finally, a full document can be replaced if it existed before.
If it didn't exist, then a `DocumentDoesNotExistException` will be thrown:

[source,java]
----
JsonDocument doc = JsonDocument.create("document_id", JsonObject.empty().put("more", "content"));
System.out.println(bucket.replace(doc));
----

....
Output: JsonDocument{id='document_id', cas=216109395083264, expiry=0, content={"more":"content"}, mutationToken=null}
....

[#java-retrieving-full-docs]
== Retrieving full documents

You can retrieve documents using the [.api]`Bucket#get()`, [.api]`Bucket#getAndLock()`, [.api]`Bucket#getAndTouch()` and [.api]``Bucket#getFromReplica()``methods.
All of those serve different distinct purposes and accept different parameters.

Most of the time you use the [.api]`get()` method.
It accepts one mandatory argument:

* [.param]`id`: The document ID to retrieve

[source,java]
----
System.out.println(bucket.get("document_id"));
----

....
Output: JsonDocument{id='document_id', cas=216109395083264, expiry=0, content={"more":"content"}, mutationToken=null}
....

Other overloads are available for advanced purposes:

* [.param]`document`: Instead of just passing an Id a full document can be passed in.
If so, the ID is extracted and used.
* [.param]`target`: A custom Document type (other than `JsonDocument`) can be specified.
* [.param]`timeout`, [.param]`timeUnit`: Specify a custom timeout which overrides the default timeout setting.

[source,java]
----
// Use a Document where ID is extracted
JsonDocument someDoc = JsonDocument.create("document_id");
System.out.println(bucket.get(someDoc));
----

....
Output: JsonDocument{id='document_id', cas=216109395083264, expiry=0, content={"more":"content"}, mutationToken=null}
....

[source,java]
----
// A custom Document type, here it returns the plain raw JSON String, encoded.
RawJsonDocument doc = bucket.get("document_id", RawJsonDocument.class);
String content = doc.content();
System.out.println(content);
----

....
Output: {"more":"content"}
....

[source,java]
----
// Wait only 1 second instead of the default timeout
JsonDocument doc = bucket.get("document_id", 1, TimeUnit.SECONDS);
----

It is also possible to read from a replica if you want to explicitly trade availability for consistency during the timeframe when the active partition is not reachable (for example during a node failure or netsplit).

`getFromReplica` has one mandatory argument as well:

* [.param]`id`: The document ID to retrieve

Since you can have 0 to 3 replicas (and they can change at runtime of your application) the `getFromReplica` returns Lists or Iterators.
It is recommended to use the Iterator APIs since they provide more flexibility during error conditions (since only partial responses may be retreived).

[source,java]
----
Iterator<JsonDocument> docIter = bucket.getFromReplica("document_id");
while(docIter.hasNext()) {
    JsonDocument replicaDoc = docIter.next();
    System.out.println(replicaDoc);
}
----

Other overloads are available for advanced purposes:

* [.param]`replicaMode`: Allows to configure from which replicas to read from (defaults to all).
* [.param]`document`: Instead of just passing an Id a full document can be passed in.
If so, the ID is extracted and used.
* [.param]`target`: A custom Document type (other than `JsonDocument`) can be specified.
* [.param]`timeout`, [.param]`timeUnit`: Specify a custom timeout which overrides the default timeout setting.

TIP: In general, always use the [.param]`ReplicaMode.ALL` option and not [.param]`ReplicaMode.FIRST` and similar to just get the first replica.
The reason is that is that ALL will also try the active node, leading to more reliable behavior during failover.
If you just need the first replica use the iterator approach and `break;` once you have enough data from the replicas.

IMPORTANT: Since a replica is updated asynchronously and eventually consistent, reading from it may return stale and/or outdated results!

If you need to use pessimistic write locking on a document you can use the [.param]`getAndLock` which will retreive the document if it exists and also return its [.param]`CAS` value.
You need to provide a time that the document is maximum locked (and the server will unlock it then) if you don't update it with the valid cas.
Also note that this is a pure write lock, reading is still allowed.

[source,java]
----
// Get and Lock for max of 10 seconds
JsonDocument ownedDoc = bucket.getAndLock("document_id", 10);

// Do something with your document
JsonDocument modifiedDoc = modifyDocument(ownedDoc);

// Write it back with the correct CAS
bucket.replace(modifiedDoc);
----

If the document is locked already and you are trying to lock it again you will receive a `TemporaryLockFailureException`.

It is also possible to fetch the document and reset its expiration value at the same time.
See xref:document-operations.adoc#java-modifying-expiration[Modifying Expiration] for more information.

[#java-removing-full-docs]
== Removing full documents

You can remove documents using the [.api]`Bucket.remove()` method.
This method takes a single mandatory argument:

* [.param]`id`: The ID of the document to remove.

Some additional options:

* [.param]`persistTo`, [.param]`replicateTo`: Specify xref:durability.adoc[durability requirements] for the operations.
* [.param]`timeout`, [.param]`timeUnit`: Specify a custom timeout which overrides the default timeout setting.

If the `cas` value is set on the Document overload, it is used to provide optimistic currency, very much like the `replace` operation.

[source,java]
----
// Remove the document
JsonDocument removed = bucket.remove("document_id");
----

[source,java]
----
JsonDocument loaded = bucket.get("document_id");

// Remove and take the CAS into account
JsonDocument removed = bucket.remove(loaded);
----

[#java-modifying-expiration]
== Modifying expiration

Many methods support setting the expiry value as part of their other primary operations:

* [.api]`Bucket#touch`: Resets the expiry time for the given document ID to the value provided.
* [.api]`Bucket#getAndTouch`: Fetches the document and resets the expiry to the given value provided.
* [.api]`Bucket#insert`, [.api]`Bucket#upsert`, [.api]`Bucket#replace`: Stores the expiry value alongside the actual mutation when set on the `Document` instance.

The following example stores a document with an expiry, waits a bit longer and as a result no document is found on the subsequent get:

[source,java]
----
int expiry = 2; // seconds
JsonDocument stored = bucket.upsert(
    JsonDocument.create("expires", expiry, JsonObject.create().put("some", "value"))
);

Thread.sleep(3000);

System.out.println(bucket.get("expires"));
----

....
null
....

You may also use the [.api]`Bucket#touch()` method to modify expiration without fetching or modifying the document:

----
bucket.touch("expires", 2);
----

[#java-atomic-modifications]
== Atomic Document Modifications

Additional atomic document modifications can be performed using the Java SDK.
You can modify a xref:core-operations.adoc#devguide_kvcore_counter_generic[counter document] using the [.api]`Bucket.counter()` method.
You can also use the [.api]`Bucket.append()` and [.api]`Bucket.prepend()` methods to perform xref:core-operations.adoc#devguide_kvcore_append_prepend_generic[raw byte concatenation].

[#java-batching-ops]
== Batching Operations

Since the Java SDK uses RxJava as its asynchronous foundation, all operations can be xref:batching-operations.adoc[batched] in the SDK using the xref:async-programming.adoc[asynchronous API] via `bucket.async()` (and optionally revert back to blocking).

For implicit batching use these operators: `Observable.just()` or `Observable.from()` to generate an observable that contains the data you want to batch on.
`flatMap()` to send those events against the Couchbase Java SDK and merge the results asynchronously.
`last()` if you want to wait until the last element of the batch is received.
`toList()` if you care about the responses and want to aggregate them easily.
If you have more than one subscriber, use `cache()` to prevent accessing the network over and over again with every subscribe.

The following example creates an observable stream of 6 keys to load in a batch, asynchronously fires off `get()` requests against the SDK (notice the `+bucket.async().get(...)+`), waits until the last result has arrived, and then converts the result into a list and blocks at the very end.
This pattern can be reused for mutations like `upsert` (as shown further down):

[source,java]
----
Cluster cluster = CouchbaseCluster.create();
Bucket bucket = cluster.openBucket();

List<JsonDocument> foundDocs = Observable
    .just("key1", "key2", "key3", "key4", "inexistentDoc", "key5")
    .flatMap(new Func1<String, Observable<JsonDocument>>() {
        @Override
        public Observable<JsonDocument> call(String id) {
            return bucket.async().get(id);
        }
    })
    .toList()
    .toBlocking()
    .single();

for (JsonDocument doc : foundDocs) {
    System.out.println(doc.id());
}
----

....
key1
key2
key3
key4
key5
....

Note that this always returns a list, but it may contain 0 to 6 documents (here 5) depending on how many are actually found.
Also, at the very end the observable is converted into a blocking one, but everything before that, including the network calls and the aggregation, is happening completely asynchronously.

Inside the SDK, this provides much more efficient resource utilization because the requests are very quickly stored in the internal Request RingBuffer and the I/O threads are able to pick batches as large as they can.
Afterward, whatever server returns a result first it is stored in the list, so there is no serialization of responses going on.

Batching mutations: The previous Java SDK only provided bulk operations for get().
With the techniques shown above, you can perform any kind of operation as a batch operation.
The following code generates a number of fake documents and inserts them in one batch.
Note that you can decide to either collect the results with `toList()` as shown above or just use `last()` as shown here to wait until the last document is properly inserted:

[source,java]
----
// Generate a number of dummy JSON documents
int docsToCreate = 100;
List<JsonDocument> documents = new ArrayList<JsonDocument>();
for (int i = 0; i < docsToCreate; i++) {
    JsonObject content = JsonObject.create()
        .put("counter", i)
        .put("name", "Foo Bar");
    documents.add(JsonDocument.create("doc-"+i, content));
}

// Insert them in one batch, waiting until the last one is done.
Observable
    .from(documents)
    .flatMap(new Func1<JsonDocument, Observable<JsonDocument>>() {
        @Override
        public Observable<JsonDocument> call(final JsonDocument docToInsert) {
            return bucket.async().insert(docToInsert);
        }
    })
    .last()
    .toBlocking()
    .single();
----

[#java-subdocs]
== Operating with Sub-Documents

TIP: Sub-Document API is available starting Couchbase Server version 4.5.
See xref:subdocument-operations.adoc[Sub-Document Operations] for an overview.

Sub-document operations save network bandwidth by allowing you to specify _paths_ of a document to be retrieved or updated.
The document is parsed on the server and only the relevant sections (indicated by _paths_) are transferred between client and server.
You can execute xref:subdocument-operations.adoc[sub-document] operations in the Java SDK using the [.api]`Bucket#lookupIn()` and [.api]`Bucket#mutateIn()` methods.

Each of these methods accepts a [.param]`key` as its mandatory first argument and give you a builder that you can use to chain several _command specifications_, each specifying the path to be impacted by the specified operation and a document field operand.
You may find all the operations in the [.api]`LookupInBuilder` and [.api]`MutateInBuilder` classes.

[source,java]
----
bucket.lookupIn("docid")
    .get("path.to.get")
    .exists("check.path.exists")
    .execute();

boolean createParents = true;
bucket.mutateIn("docid")
    .upsert("path.to.upsert", value, createParents)
    .remove("path.to.del"))
    .execute();
----

All sub-document operations return a special [.api]`DocumentFragment` object rather than a [.api]`Document`.
It shares the `id()`, `cas()` and `mutationToken()` fields of a document, but in contrast with a normal [.api]`Document` object, a [.api]`DocumentFragment` object contains multiple results with multiple statuses, one result/status pair for every input operation.
So it exposes method to get the `content()` and `status()` of each spec, either by index or by path.
It also allows to check that a response for a particular spec `exists()`:

[source,java]
----
DocumentFragment<Lookup> res =
bucket.lookupIn("docid")
    .get("foo")
    .exists("bar")
    .exists("baz")
    .execute();

// First result
res.content("foo");
// or
res.content(0);
----

Using the `+content(...)+` methods will raise an exception if the individual spec did not complete successfully.
You can also use the `+status(...)+` methods to return an error code (a [.api]`ResponseStatus`) rather than throw an exception.

[#java-formats-non-json]
== Formats and Non-JSON Documents

TIP: See xref:nonjson.adoc[Non-JSON Documents] for a general overview of using non-JSON documents with Couchbase

The Java SDK defines several concrete implementations of a [.api]`Document` to represent the various data types that it can store.
Here is the complete list of document types:

.Documents with JSON content
|===
| Document Name | Description

| [.api]`JsonDocument`
| The default, which has a [.api]`JsonObject` at the top level content.

| [.api]`RawJsonDocument`
| Stores any JSON value and should be used if custom JSON serializers such as Jackson or GSON are already in use.

| [.api]`JsonArrayDocument`
| Similar to JsonDocument, but has a [.api]`JsonArray` at the top level content.

| [.api]`JsonBooleanDocument`
| Stores JSON-compatible Boolean values.

| [.api]`JsonLongDocument`
| Stores JSON compatible long (number) values.

| [.api]`JsonDoubleDocument`
| Stores JSON compatible double (number) values.

| [.api]`JsonStringDocument`
| Stores JSON compatible [.api]`String` values.
Input is automatically wrapped with quotes when stored.

| [.api]`EntityDocument`
| Used with the [.api]`Repository` implementation to write and read POJOs into JSON and back.
|===

.Documents with other content
|===
| Document Name | Description

| [.api]`BinaryDocument`
| Can be used to store arbitrary binary data.

| [.api]`SerializableDocument`
| Stores objects that implement [.api]`Serializable` through default Java object serialization.

| [.api]`LegacyDocument`
| Uses the [.api]`Transcoder` from the 1.x SDKs and can be used for full cross-compatibility between the old and new versions.

| [.api]`StringDocument`
| Can be used to store arbitrary strings.
They will not be quoted, but stored as-is and flagged as "String".
|===

You can implement a custom document type and associated transcoder if none of the pre-configured options are suitable for your application.
A custom transcoder converts intputs to their serialized forms, and deserializes encoded data based on the item flags.
There is an [.api]`AbstractTranscoder` that can serve as the basis for a custom implementation, and custom transcoders should be registered with a [.api]`Bucket` when calling [.api]`Cluster#openBucket` (a list of custom transcoders can be passed in one of the overloads).

[#java-binary-document]
== Correctly Managing BinaryDocuments

The `BinaryDocument` can be used to store and read arbitrary bytes.
It is the only default codec that directly exposes the underlying low-level Netty `ByteBuf` objects.

IMPORTANT: Because the raw data is exposed, it is important to free it after it has been properly used.
Not freeing it will result in increased garbage collection and memory leaks and should be avoided by all means.
See <<binary-memory>>.

Because binary data is arbitrary anyway, it is backward compatible with the old SDK regarding flags so that it can be read and written back and forth.
Make sure it is not compressed in the old SDK and that the same encoding and decoding process is used on the application side to avoid data corruption.

Here is some demo code that shows how to write and read raw data.
The example writes binary data, reads it back, and then frees the pooled resources:

[source,java]
----
// Create buffer out of a string
ByteBuf toWrite = Unpooled.copiedBuffer("Hello World", CharsetUtil.UTF_8);

// Write it
bucket.upsert(BinaryDocument.create("binaryDoc", toWrite));

// Read it back
BinaryDocument read = bucket.get("binaryDoc", BinaryDocument.class);

// Print it
System.out.println(read.content().toString(CharsetUtil.UTF_8));

// Free the resources
ReferenceCountUtil.release(read.content());
----

[#binary-memory]
== Correctly Managing Buffers

`BinaryDocument` allows users to get the rawest form of data out of Couchbase.
It  exposes Netty's `ByteBuf`, byte buffers that can have various characteristics (on- or off-heap, pooled or unpooled).
In general, buffers created by the SDK are pooled and off heap.
You can disable the pooling in the `CouchbaseEnvironment` if you absolutely need that.

As a consequence, the memory associated with the ByteBuf must be a little bit more managed by the developer than usual in Java.

Most notably, these byte buffers are reference counted, and you need to know three main methods associated to buffer management:

* `refCnt()` gives you the current reference count.
When it hits 0, the buffer is released back to its original pool, and it cannot be used anymore.
* `release()` will decrease the reference count by 1 (by default).
* `retain()` is the inverse of release, allowing you to prepare for multiple consumptions by external methods that you know will each release the buffer.

You can also use `ReferenceCountUtil.release(something)` if you don't want to check if `something` is actually a `ByteBuf` (will do nothing if it's not something that is [.api]`ReferenceCounted`).

IMPORTANT: The SDK bundles the Netty dependency into a different package so that it doesn't clash with a dependency to another version of Netty you may have.
As such, you need to use the classes and packages provided by the SDK (`com.couchbase.client.deps.io.netty`) when interacting with the API.
For example, the `ByteBuf` for the content of a `BinaryDocument` is a `com.couchbase.client.deps.io.netty.buffer.ByteBuf`.

*What happens if I don't release?*

Basically, you leak memory\... Netty will by default inspect a small percentage of `ByteBuf` creations and usage to try and detect leaks (in which case it will output a log, look for the "LEAK" keyword).

You can tune that to be more eagerly monitoring all buffers by calling `ResourceLeakDetector.setLevel(PARANOID)`.

IMPORTANT: Note that this incurs quite an overhead and should only be activated in tests.
In production (prod), setting it to `ADVANCED` is not as heavy as paranoid and can be a good middle ground.

*What happens if I release twice (or the SDK releases once more after I do)?*

Netty will throw `IllegalReferenceCountException`.
The buffer that has RefCnt = 0 cannot be interacted with anymore since it means it has been freed back into the pool.

*When must I release?*

When the SDK creates a `BinaryDocument` for you, basically GET-type operations.

Mutative operations, on the other hand, will take care of the buffer you pass in for you, at the time the buffer is written on the wire.

*When must I usually retain?*

When you do a write, the buffer will usually be released by the SDK calling `release()`.
But if you implement a kind of fallback behavior (for instance attempt to `insert()` a doc, catch `DocumentAlreadyExistException` and then fallback to an `update()` instead), that means the SDK would attempt to release twice, which won't work.

In this case you can `retain()` the buffer before the first attempt, let the catch block do the extra release if something goes wrong.
You have to manage the extra release if the first write succeeds, and think about catching other possible exceptions (here also an extra release is needed):

[source,java]
----
byteBuffer.retain(); //prepare for potential multi usage (+1 refCnt, refCnt = 2)
try {
   bucket.append(document);
   // refCnt = 2 on success
   byteBuffer.release(); //refCnt = 1
} catch (DocumentDoesNotExistException dneException) {
   // buffer is released on errors, refCnt = 1
   //second usage will also release, but we want to be at refCnt = 1 for the finally block
   byteBuffer.retain(); //refCnt = 2
   bucket.insert(document); //refCnt = 1
} // other uncaught errors will still cause refCnt to be released down to 1
finally {
   //we made sure that at this point refCnt = 1 in any case (success, caught exception, uncaught exception)
   byteBuffer.release(); //refCnt = 0, returned to the pool
}
----
